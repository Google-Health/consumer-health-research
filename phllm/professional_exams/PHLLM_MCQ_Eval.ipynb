{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrwVQsM9TiUw"
      },
      "source": [
        "##### Copyright 2025 Google LLC.\n",
        "Licensed under the Apache 2.0 License."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpDUTVKYTowI"
      },
      "source": [
        "# @title Licensed under the Apache 2.0 License (the \"License\"); { display-mode: \"form\" }\n",
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtT_DjY6U7hZ"
      },
      "source": [
        "## PH-LLM Professional Exam Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWJsnW4MfoGj"
      },
      "outputs": [],
      "source": [
        "# @title Import\n",
        "import collections\n",
        "from collections import Counter\n",
        "import re\n",
        "import time\n",
        "from typing import Any, Callable, List, Optional, Set, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from saxml.client.python import sax\n",
        "import copy\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr3ENUgQu-EB"
      },
      "outputs": [],
      "source": [
        "# @title Utilities and MCQ processing.\n",
        "def _get_sax_model(\n",
        "    sax_address,\n",
        "    use_proxy=False,\n",
        "    num_conn: int = 1,\n",
        ") -\u003e sax.Model.LM:\n",
        "  \"\"\"Get a SAX model.\"\"\"\n",
        "  opts = sax.Options()\n",
        "  if use_proxy:\n",
        "    opts.proxy_addr = 'sax_proxy_address' # @param {type: \"string\"}\n",
        "  opts.num_conn = num_conn\n",
        "  return sax.Model(sax_address, opts).LM()\n",
        "\n",
        "\n",
        "def find_majority_vote_answer(\n",
        "    dict_list: List[Tuple[str, Set[str]]]\n",
        ") -\u003e Optional[Tuple[str, Set[str]]]:\n",
        "  \"\"\"Finds the tuple with the majority vote from a list of tuples.\"\"\"\n",
        "  # Extract identifiers and count them\n",
        "  identifiers = []\n",
        "  for entry in dict_list:\n",
        "    entry_answer = entry['model_answer']\n",
        "    if entry_answer not in ['(A)', '(B)', '(C)', '(D)', '(E)']:\n",
        "      continue\n",
        "    else:\n",
        "      identifiers.append(entry_answer)\n",
        "  counts = Counter(identifiers)\n",
        "  most_common = counts.most_common(2)\n",
        "  # Check for a clear majority\n",
        "  if len(most_common) == 1 or (\n",
        "      len(most_common) \u003e 1 and most_common[0][1] \u003e most_common[1][1]\n",
        "  ):\n",
        "    majority_identifier = most_common[0][0]\n",
        "    # Find and return the first tuple with the majority identifier\n",
        "    for t in dict_list:\n",
        "      if t['model_answer'] == majority_identifier:\n",
        "        return (t['model_answer'], t['model_generations'])\n",
        "  # If no majority or a tie, return None\n",
        "  return None\n",
        "\n",
        "\n",
        "def _postprocess_generation_answer(generations: set[str]) -\u003e str:\n",
        "  \"\"\"Process the generated answers.\"\"\"\n",
        "  answer_re = re.compile(r'\u003canswer\u003e(\\([ABCDE]\\))\u003c/answer\u003e', re.IGNORECASE)\n",
        "  answers = []\n",
        "  for gen in generations:\n",
        "    gen = gen.strip()\n",
        "    matcher = answer_re.search(gen)\n",
        "    if matcher:\n",
        "      answers.append(matcher.group(1).upper())\n",
        "  # If no generation yielded a valid formatted Answer, flag as skipped.\n",
        "  if not answers:\n",
        "    return {_MODEL_GEN: generations, _SKIPPED: 1}\n",
        "  # This extracts a list of most common answers within the xml tags\n",
        "  # \u003canswer\u003e\u003c/answer\u003e, takes the first entry (in case of ties), and then\n",
        "  # extracts the answer text (the second entry in the pair is the number of\n",
        "  # times it appeared).\n",
        "  model_answer = collections.Counter(answers).most_common(1)[0][0]\n",
        "  return model_answer\n",
        "\n",
        "def add_instruction_to_prompt(\n",
        "    samples: list[dict[str, Any]], domain: str\n",
        ") -\u003e list[dict[str, Any]]:\n",
        "  \"\"\"Returns samples with `inputs` modified to add instruction to prompt.\"\"\"\n",
        "  if domain == 'Sleep':\n",
        "    input_key = mcq_constants.SLEEP_MCQ_INPUTS_FEATURE_NAME\n",
        "    choices_key = mcq_constants.SLEEP_MCQ_EVAL_LABELS_FEATURE_NAME\n",
        "  elif domain == 'Fitness':\n",
        "    input_key = mcq_constants.FITNESS_MCQ_INPUTS_FEATURE_NAME\n",
        "    choices_key = mcq_constants.FITNESS_MCQ_EVAL_LABELS_FEATURE_NAME\n",
        "  else:\n",
        "    raise ValueError(f'Invalid domain: {domain}')\n",
        "\n",
        "  retval = []\n",
        "  for orig_sample in samples:\n",
        "    sample = copy.deepcopy(orig_sample)\n",
        "    sample[input_key] = mcq_prompt_lib.create_prompt_to_generate_mcqs(\n",
        "        sample[input_key], sample[choices_key], domain\n",
        "    )\n",
        "    retval.append(sample)\n",
        "  return retval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SJExEAd6ZUP"
      },
      "outputs": [],
      "source": [
        "def read_mcq_dataset(\n",
        "    dataset_path: str, domain: str, difficulty_level: Optional[list[str]] = None\n",
        ") -\u003e pd.DataFrame:\n",
        "  \"\"\"Reads the MCQ dataset.\"\"\"\n",
        "  with open(dataset_path, 'r') as f:\n",
        "    synthetic_mcq_dataset = pd.read_csv(f)\n",
        "\n",
        "  synthetic_mcq_dataset['choices'] = synthetic_mcq_dataset['choices'].apply(\n",
        "      ast.literal_eval\n",
        "  )\n",
        "\n",
        "  if difficulty_level:\n",
        "    synthetic_mcq_dataset = synthetic_mcq_dataset[\n",
        "        (synthetic_mcq_dataset['domain'] == domain)\n",
        "        \u0026 (synthetic_mcq_dataset['difficulty'].isin(difficulty_level))\n",
        "    ]\n",
        "  else:\n",
        "    synthetic_mcq_dataset = synthetic_mcq_dataset[\n",
        "        synthetic_mcq_dataset['domain'] == domain\n",
        "    ]\n",
        "  return synthetic_mcq_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpQ_DRcwAbXA"
      },
      "outputs": [],
      "source": [
        "# @title Model Evaluation\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    sax_address: str,\n",
        "    dataset_path: str,\n",
        "    domain: str,\n",
        "    eval_func: Callable[\n",
        "        [\n",
        "            sax.LanguageModel,\n",
        "            str,\n",
        "            dict[str, str],\n",
        "            Optional[str],\n",
        "            Optional[float],\n",
        "            Optional[int],\n",
        "            Optional[int],\n",
        "        ],\n",
        "        dict[str, int],\n",
        "    ],\n",
        "    num_examples: int = -1,\n",
        "    num_replicas: int = 1,\n",
        "    prompt_type: Optional[str] = None,\n",
        "    use_proxy: bool = False,\n",
        "    temperature: float = 0.0,\n",
        "    max_decoding_steps: int = 2048,\n",
        "    sc_round: int = 1,\n",
        "    mcq_difficulty_level: Optional[list[str]] = None,\n",
        ") -\u003e list[dict[str, Any]]:\n",
        "  \"\"\"Returns counts of 'correct', 'incorrect', 'skipped' questions.\n",
        "\n",
        "  Args:\n",
        "    sax_address: Path to the SAX model address.\n",
        "    dataset_path: Path to the dataset of MCQ example questions.\n",
        "    domain: The domain of the MCQ dataset (e.g. 'sleep' or 'fitness').\n",
        "    eval_func: Function used to evaluate the model specified at `sax_address`.\n",
        "    num_examples: Number of examples to evaluate. If \u003c0, evaluates all examples.\n",
        "    num_replicas: Number of model replicas available. To parallelize, we need to\n",
        "      both specify the number of connections to open to the server and then run\n",
        "      parallel evaluations on the model.\n",
        "    prompt_type: The type of prompt to use (e.g., CoT or Step-Back).\n",
        "    use_proxy: Whether to use the proxy to connect to the model.\n",
        "    temperature: The temperature to use for the llm model.\n",
        "    max_decoding_steps: The maximum number of decoding steps to run.\n",
        "    sc_round: The round of self-consistency.\n",
        "    mcq_difficulty_level: The difficulty level of the MCQ dataset.\n",
        "\n",
        "  Returns:\n",
        "    A list of the examples featurized as dictionaries along with the model\n",
        "    results.\n",
        "  \"\"\"\n",
        "  start_time = time.time()\n",
        "  model = _get_sax_model(\n",
        "      sax_address, num_conn=num_replicas, use_proxy=use_proxy\n",
        "  )\n",
        "  if domain == 'Sleep':\n",
        "    feature_dicts = read_mcq_dataset(\n",
        "        dataset_path,\n",
        "        domain=domain,\n",
        "        difficulty_level=mcq_difficulty_level,\n",
        "    )\n",
        "  elif domain == 'Fitness':\n",
        "    feature_dicts = read_mcq_dataset(\n",
        "        dataset_path, domain=domain, difficulty_level='None'\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(f'Unknown domain: {domain}')\n",
        "  if num_examples \u003c 0:\n",
        "    num_examples = len(feature_dicts)\n",
        "  examples_to_evaluate = feature_dicts[:num_examples]\n",
        "  def _run_one_example(feats: dict[str, Any]) -\u003e dict[str, Any]:\n",
        "    inputs = feats.copy()\n",
        "    res = eval_func(\n",
        "        model=model,\n",
        "        domain=domain,\n",
        "        features=inputs,\n",
        "        prompt_type=prompt_type,\n",
        "        temperature=temperature,\n",
        "        max_decoding_steps=max_decoding_steps,\n",
        "        sc_round=sc_round,\n",
        "    )\n",
        "    assert set(res.keys()).isdisjoint(set(inputs.keys()))\n",
        "    res.update(inputs)\n",
        "    return res\n",
        "  retval = []\n",
        "  for _, ex in examples_to_evaluate.iterrows():\n",
        "    retval.append({'feats': ex.to_dict()})\n",
        "  print(\n",
        "      f'Evaluated {dataset_path} with {sax_address} in'\n",
        "      f' {time.time() - start_time} seconds using {num_replicas} workers.',\n",
        "      flush=True,\n",
        "  )\n",
        "  return retval\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Methods for evaluating MCQs.\n",
        "################################################################################\n",
        "\n",
        "# Potential outcomes from evaluating the model on the question.\n",
        "_CORRECT = 'correct'\n",
        "_INCORRECT = 'incorrect'\n",
        "_SKIPPED = 'skipped'\n",
        "_NO_MAJORITY_VOTE = {'NO MAJORITY VOTE, USED lm.Score INSTEAD'}\n",
        "\n",
        "# The answer the model provided (if not _SKIPPED).\n",
        "_MODEL_ANSWER = 'model_answer'\n",
        "\n",
        "# Relevant only for lm.Score -- the raw logprobs of each choice.\n",
        "_MODEL_SCORES = 'model_scores'\n",
        "\n",
        "# Relevant only for lm.Generate -- the generated text.\n",
        "_MODEL_GEN = 'model_generations'\n",
        "\n",
        "\n",
        "def eval_score(\n",
        "    *,\n",
        "    model: sax.LanguageModel,\n",
        "    domain: str,\n",
        "    features: dict[str, Any],\n",
        "    prompt_type: Optional[str] = None,\n",
        "    temperature: float = 0.0,\n",
        "    max_decoding_steps: int = 5,\n",
        "    sc_round=None,\n",
        ") -\u003e dict[str, Any]:\n",
        "  \"\"\"Returns correct/incorrect for the question when evaluated with lm.Score.\"\"\"\n",
        "  del sc_round  # unused.\n",
        "  del prompt_type  # unused.\n",
        "  del temperature  # unused.\n",
        "  del max_decoding_steps  # unused.\n",
        "  full_question = add_instruction_to_prompt(\n",
        "      [features], domain=domain\n",
        "  )[0]['question']\n",
        "\n",
        "  # Run lm.Score for the question.\n",
        "  if isinstance(model, sax.LanguageModel):\n",
        "    sax_options = sax.ModelOptions()\n",
        "    sax_options.SetTimeout(150)\n",
        "    scores = []\n",
        "    for ao in features['choices']:\n",
        "      scores.extend(model.Score(full_question, [ao], sax_options))\n",
        "  else:\n",
        "    raise ValueError(f'Unsupported model type: {type(model)}')\n",
        "  model_answer = list(features['choices'].keys())[np.argmax(scores)]\n",
        "  return {\n",
        "      _MODEL_ANSWER: model_answer,\n",
        "      _MODEL_SCORES: scores,\n",
        "      _CORRECT if model_answer == features['answer'] else _INCORRECT: 1,\n",
        "  }\n",
        "\n",
        "\n",
        "def _create_mcq_generate_prompt(\n",
        "    mcq_question: str,\n",
        "    mcq_options: dict[str, str],\n",
        "    prompt_type: str,\n",
        "    domain: str,\n",
        ") -\u003e str:\n",
        "  \"\"\"Converts a sleep MCQ question to a generate prompt.\"\"\"\n",
        "  if prompt_type == 'step_back' and domain == 'Sleep':\n",
        "    return mcq_prompt_lib.SLEEP_TAKE_STEP_BACK_MCQ.format(\n",
        "        mcq_options=', '.join(sorted(mcq_options)),\n",
        "        mcq_question=mcq_question.strip(),\n",
        "        domain=domain,\n",
        "    )\n",
        "  elif prompt_type == 'cot' and domain == 'Sleep':\n",
        "    return mcq_prompt_lib.SLEEP_COT_MCQ.format(\n",
        "        mcq_options=', '.join(sorted(mcq_options)),\n",
        "        mcq_question=mcq_question.strip(),\n",
        "        domain=domain,\n",
        "    )\n",
        "  elif prompt_type == 'cot' and domain == 'Fitness':\n",
        "    return mcq_prompt_lib.FITNESS_COT_MCQ.format(\n",
        "        mcq_options=', '.join(sorted(mcq_options)),\n",
        "        mcq_question=mcq_question.strip(),\n",
        "        domain=domain,\n",
        "    )\n",
        "  elif prompt_type == 'step_back' and domain == 'Fitness':\n",
        "    return mcq_prompt_lib.FITNESS_TAKE_STEP_BACK_MCQ.format(\n",
        "        mcq_options=', '.join(sorted(mcq_options)),\n",
        "        mcq_question=mcq_question.strip(),\n",
        "        domain=domain,\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        f'Unsupported combination of prompt type and domain: {prompt_type} and'\n",
        "        f' {domain=}.'\n",
        "    )\n",
        "\n",
        "\n",
        "def eval_generate(\n",
        "    *,\n",
        "    model: sax.LanguageModel,\n",
        "    domain: str,\n",
        "    features: dict[str, str],\n",
        "    prompt_type: Optional[str] = None,\n",
        "    temperature: float = 0.0,\n",
        "    max_decoding_steps: int = 2048,\n",
        "    sc_round=None,\n",
        ") -\u003e dict[str, Any]:\n",
        "  \"\"\"Returns correct/incorrect for the question when evaluated with lm.Generate.\"\"\"\n",
        "  del sc_round  # unused.\n",
        "  full_question = _create_mcq_generate_prompt(\n",
        "      features['question'],\n",
        "      features['choices'],\n",
        "      prompt_type,\n",
        "      domain,\n",
        "  )\n",
        "  if isinstance(model, sax.LanguageModel):\n",
        "    sax_options = sax.ModelOptions()\n",
        "    # N.B.: Could consider modifying this. Right now this does multiple\n",
        "    # generations and aggregates. But it could be changed to only do the most\n",
        "    # likely (temperature=0) or other values.\n",
        "    sax_options.SetExtraInput('temperature', temperature)\n",
        "    sax_options.SetExtraInput(\n",
        "        'per_example_max_decode_steps', max_decoding_steps\n",
        "    )\n",
        "    sax_options.SetTimeout(150)\n",
        "    # Uniquify all the different generations created.\n",
        "    generations = {gen for gen, _ in model.Generate(full_question, sax_options)}\n",
        "  else:\n",
        "    raise ValueError(f'Unsupported model type: {type(model)}')\n",
        "  model_answer = _postprocess_generation_answer(generations)\n",
        "  return {\n",
        "      _MODEL_ANSWER: model_answer,\n",
        "      _CORRECT if model_answer == features['answer'] else _INCORRECT: 1,\n",
        "      _MODEL_GEN: generations,\n",
        "  }\n",
        "\n",
        "\n",
        "def eval_generate_sc(\n",
        "    *,\n",
        "    model: sax.LanguageModel,\n",
        "    domain: str,\n",
        "    features: dict[str, str],\n",
        "    prompt_type: Optional[str] = None,\n",
        "    temperature: float = 0.0,\n",
        "    max_decoding_steps: int = 2048,\n",
        "    sc_round: int = 5,\n",
        ") -\u003e dict[str, Any]:\n",
        "  \"\"\"Returns correct/incorrect for the question when evaluated with lm.Generate.\"\"\"\n",
        "  sc_generations = [\n",
        "      eval_generate(\n",
        "          model=model,\n",
        "          domain=domain,\n",
        "          features=features,\n",
        "          prompt_type=prompt_type,\n",
        "          temperature=temperature,\n",
        "          max_decoding_steps=max_decoding_steps,\n",
        "          sc_round=None,\n",
        "      )\n",
        "      for _ in range(sc_round)\n",
        "  ]\n",
        "  most_popular_answer = find_majority_vote_answer(sc_generations)\n",
        "  if not most_popular_answer:\n",
        "    retval = eval_score(\n",
        "        model=model,\n",
        "        domain=domain,\n",
        "        features=features,\n",
        "        prompt_type=None,\n",
        "        temperature=temperature,\n",
        "        max_decoding_steps=max_decoding_steps,\n",
        "        sc_round=sc_round,\n",
        "    )\n",
        "    del retval[_MODEL_SCORES]\n",
        "    retval[_MODEL_GEN] = _NO_MAJORITY_VOTE\n",
        "    return retval\n",
        "  else:\n",
        "    model_answer, generations = most_popular_answer\n",
        "  return {\n",
        "      _MODEL_ANSWER: model_answer,\n",
        "      _CORRECT if model_answer == features['answer'] else _INCORRECT: 1,\n",
        "      _MODEL_GEN: generations,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaydEMlcobwM"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "005JRqOfPaXl"
      },
      "outputs": [],
      "source": [
        "def _accuracy(results: list[dict[str, Any]]) -\u003e tuple[int, int, float]:\n",
        "  \"\"\"Returns (correct, incorrect, accuracy) tuple.\"\"\"\n",
        "  correct = sum(q.get(_CORRECT, 0) for q in results)\n",
        "  incorrect = sum(q.get(_INCORRECT, 0) for q in results)\n",
        "  acc = np.nan if correct + incorrect == 0 else correct / (correct + incorrect)\n",
        "  return correct, incorrect, acc\n",
        "\n",
        "\n",
        "def analyze_results(results: list[dict[str, Any]]) -\u003e None:\n",
        "  \"\"\"Prints out analysis of results, both stratified and combined.\"\"\"\n",
        "  stratifications = {'All': results}\n",
        "  num_questions_with_difficulty = sum(int('difficulty' in q) for q in results)\n",
        "  if num_questions_with_difficulty not in [0, len(results)]:\n",
        "    raise ValueError(\n",
        "        'Expected either all or none of the questions to be annotated with '\n",
        "        f'difficulty, found {num_questions_with_difficulty}/{len(results)}.'\n",
        "    )\n",
        "  if num_questions_with_difficulty:\n",
        "    for difficulty in {q['difficulty'] for q in results}:\n",
        "      stratifications[difficulty] = [\n",
        "          q for q in results if q['difficulty'] == difficulty\n",
        "      ]\n",
        "\n",
        "  for diff, strat in sorted(stratifications.items()):\n",
        "    correct, incorrect, acc = _accuracy(strat)\n",
        "    print(\n",
        "        f'Accuracy for {diff} questions: {correct}/{correct + incorrect} ='\n",
        "        f' {acc:.2f}'\n",
        "    )\n",
        "\n",
        "\n",
        "def save_results(results: list[dict[str, Any]], filename: str) -\u003e None:\n",
        "  \"\"\"Saves results to a CSV file.\"\"\"\n",
        "  df_results = pd.DataFrame(results)\n",
        "  with open(filename, 'w') as f:\n",
        "    df_results.to_csv(f, index=True)\n",
        "\n",
        "\n",
        "def perform_full_evaluation(\n",
        "    *,\n",
        "    sax_address: str,\n",
        "    dataset_path: str,\n",
        "    domain: str,\n",
        "    outroot: str | None = None,\n",
        "    num_examples: int = -1,\n",
        "    num_replicas: int = 1,\n",
        "    prompt_type: Optional[str] = None,\n",
        "    use_proxy: bool = False,\n",
        "    use_eval_generate: bool = False,\n",
        "    temperature: float = 0,\n",
        "    max_decoding_steps: int = 2048,\n",
        "    mcq_difficulty_level: Optional[list[str]] = None,\n",
        "    sc_round: Optional[int] = None,\n",
        ") -\u003e None:\n",
        "  \"\"\"Performs full evaluation.\"\"\"\n",
        "  if outroot:\n",
        "    outroot += f'.{sax_address.split(\"/\")[-1]}'\n",
        "\n",
        "  if not use_eval_generate:\n",
        "\n",
        "    score_test_results = evaluate_model(\n",
        "        sax_address=sax_address,\n",
        "        dataset_path=dataset_path,\n",
        "        domain=domain,\n",
        "        eval_func=eval_score,\n",
        "        num_examples=num_examples,\n",
        "        num_replicas=num_replicas,\n",
        "        use_proxy=use_proxy,\n",
        "        mcq_difficulty_level=mcq_difficulty_level,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    print('## Results for lm.Score evaluation. ##')\n",
        "    print('\\n# Test data:')\n",
        "    analyze_results(score_test_results)\n",
        "    if outroot:\n",
        "      score_test_save_path = outroot.format(split='test') + '.score.csv'\n",
        "      save_results(score_test_results, score_test_save_path)\n",
        "      return score_test_results\n",
        "  elif use_eval_generate and sc_round:\n",
        "    generate_test_results = evaluate_model(\n",
        "        sax_address=sax_address,\n",
        "        dataset_path=dataset_path,\n",
        "        domain=domain,\n",
        "        eval_func=eval_generate_sc,\n",
        "        num_examples=num_examples,\n",
        "        num_replicas=num_replicas,\n",
        "        prompt_type=prompt_type,\n",
        "        use_proxy=use_proxy,\n",
        "        temperature=temperature,\n",
        "        max_decoding_steps=max_decoding_steps,\n",
        "        sc_round=sc_round,\n",
        "        mcq_difficulty_level=mcq_difficulty_level,\n",
        "    )\n",
        "    print(f'## Results for self-consistency {prompt_type} evaluation. ##')\n",
        "    print('\\n# Test data:')\n",
        "    analyze_results(generate_test_results)\n",
        "    if outroot:\n",
        "      generate_test_save_path = (\n",
        "          outroot.format(split='test') + f'.{prompt_type}.sc.csv'\n",
        "      )\n",
        "      save_results(generate_test_results, generate_test_save_path)\n",
        "    return generate_test_results\n",
        "  else:\n",
        "    generate_test_results = evaluate_model(\n",
        "        sax_address=sax_address,\n",
        "        dataset_path=dataset_path,\n",
        "        domain=domain,\n",
        "        eval_func=eval_generate,\n",
        "        num_examples=num_examples,\n",
        "        num_replicas=num_replicas,\n",
        "        prompt_type=prompt_type,\n",
        "        use_proxy=use_proxy,\n",
        "        temperature=temperature,\n",
        "        max_decoding_steps=max_decoding_steps,\n",
        "        mcq_difficulty_level=mcq_difficulty_level,\n",
        "    )\n",
        "    print(f'## Results for lm.Generate {prompt_type} evaluation. ##')\n",
        "    print('\\n# Test data:')\n",
        "    analyze_results(generate_test_results)\n",
        "    if outroot:\n",
        "      generate_test_save_path = (\n",
        "          outroot.format(split='test') + f'.{prompt_type}.csv'\n",
        "      )\n",
        "      save_results(generate_test_results, generate_test_save_path)\n",
        "    return generate_test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgXqmSD0hLRj"
      },
      "outputs": [],
      "source": [
        "SAX_ADDRESSES = [\n",
        "    'YOUR_SAX_ADDRESS'\n",
        "]\n",
        "g_num_model_replicas = 5  # @param {type:\"integer\"}\n",
        "g_domain = 'Sleep'  # @param ['Sleep', 'Fitness']\n",
        "temperature = 0.7  # @param {type:\"number\"}\n",
        "max_decoding_steps = 2048  # @param {type:\"integer\"}\n",
        "max_decoding_steps_score = 5\n",
        "sc_round = 3  # @param {type:\"integer\"}\n",
        "dataset_path = './synthetic_mcq_data.csv' # @param\n",
        "outroot = '/tmp/' # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo9fjQ8j2rVv"
      },
      "source": [
        "## CoT + Self Consistency - LM.Generate / Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdCloL-Ix4E-"
      },
      "outputs": [],
      "source": [
        "for sax_address in SAX_ADDRESSES:\n",
        "  _ = perform_full_evaluation(\n",
        "      sax_address=sax_address,\n",
        "      dataset_path=dataset_path,\n",
        "      domain=g_domain, # or 'Fitness'\n",
        "      outroot=outroot,\n",
        "      # num_examples=3, # Only used for debugging.\n",
        "      num_replicas=g_num_model_replicas,\n",
        "      prompt_type='cot',\n",
        "      use_proxy=True,\n",
        "      use_eval_generate=True,\n",
        "      temperature=temperature,\n",
        "      max_decoding_steps=max_decoding_steps,\n",
        "      mcq_difficulty_level=[\n",
        "          mcq_constants.SLEEP_MCQ_DIFF_LEVEL_EASY,\n",
        "          mcq_constants.SLEEP_MCQ_DIFF_LEVEL_MODERATE,\n",
        "          mcq_constants.SLEEP_MCQ_DIFF_LEVEL_HARD,\n",
        "      ],\n",
        "      sc_round=sc_round,\n",
        "  )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "phllm/colabs/Sleep_MCQ_Evaluation.ipynb",
          "timestamp": 1721846119161
        },
        {
          "file_id": "17TV61f-oec2TgJ3noJjSw2p8qQQ8EZBa",
          "timestamp": 1705883012466
        },
        {
          "file_id": "1LQzE6liyyIlWaimZY65OB-5sRFmGpFU3",
          "timestamp": 1705878004644
        },
        {
          "file_id": "phllm/colabs/Sleep_MCQ_Evaluation.ipynb",
          "timestamp": 1705789529171
        },
        {
          "file_id": "1ZB6QbSF2zkdsP_fYNUELLB5zB3Xya7q-",
          "timestamp": 1704217291474
        },
        {
          "file_id": "phllm/colabs/Sleep_MCQ_Evaluation.ipynb",
          "timestamp": 1701923968069
        },
        {
          "file_id": "phllm/colabs/[Benchmark]Sleep_MCQ_Evaluation_w_SAX.ipynb",
          "timestamp": 1699909204266
        },
        {
          "file_id": "phllm/colabs/[Benchmark]Sleep_MCQ_Evaluation_w_SAX.ipynb",
          "timestamp": 1699241447723
        },
        {
          "file_id": "1Lp3O8BxktAm5RNTThoHxV05kKOZN1L8x",
          "timestamp": 1698970731249
        },
        {
          "file_id": "14MsmHAGNYMQnbcmJ8e4AlapMy_OGrSwk",
          "timestamp": 1698943122736
        },
        {
          "file_id": "1Plg3CmxXliA9N7nuHZxaehJKt4mSo8dj",
          "timestamp": 1698880037422
        },
        {
          "file_id": "12d2rX1smVQb8Hj44CPxY8872WVs2RBVX",
          "timestamp": 1698858038991
        },
        {
          "file_id": "1b8etzPVJ5O_C-oCqkpwAncSfPzvF7aZ6",
          "timestamp": 1698771181969
        },
        {
          "file_id": "1qydk6_5J16gvv_qZgvwGgJjGFoC07uB6",
          "timestamp": 1697045490482
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
