{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrwVQsM9TiUw"
      },
      "source": [
        "##### Copyright 2025 Google LLC.\n",
        "Licensed under the Apache 2.0 License."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpDUTVKYTowI"
      },
      "source": [
        "# @title Licensed under the Apache 2.0 License (the \"License\"); { display-mode: \"form\" }\n",
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZTdYJQDyZzy"
      },
      "source": [
        "# Implementation of a data adapter for multimodal LLM input\n",
        "\n",
        "One way to train an LLM to use non-text data is to feed that data into the LLM's\n",
        "context window via an adapter. When you input natural language text into an LLM,\n",
        "it first gets transformed into tokens and then token embeddings. Each token\n",
        "embedding is a one-dimensional tensor of size D. This results in an input of K\n",
        "tokens mapping to a K by D matrix of inputs to the transfomer part of the LLM.\n",
        "\n",
        "Given this, a simple way to inject non-text data into the input stream of the\n",
        "LLM is to use an adapter to map the non-text data into a set of vectors of size\n",
        "D, which represent virtual tokens. For example, let's say we have a matrix\n",
        "dataset of dimension 10 by 100 that represents some features relevant to a\n",
        "single input to the LLM (a single sample). We feed the 10x100 matrix into an\n",
        "adapter that maps it into a T by D matrix, where T is some number of virtual\n",
        "tokens. Then we simply prepend this to our input matrix yielding a (K+T) by D\n",
        "matrix, which is input to the LLM.\n",
        "\n",
        "For training and evaluation of PROs in the PH-LLM paper (https://arxiv.org/abs/2406.06474), each input sample had a\n",
        "data input of size 40, which represents the concatenation of 20 mean values and\n",
        "20 variance values for 20 sensors. In this case, we learned an MLP adapter\n",
        "(implemented below) that maps the 1D tensor of length 40 to a 10 by 128\n",
        "dimensional matrix of virtual token embeddings. We then concatenated the virtual\n",
        "token matrix to the input matrix for the LLM.\n",
        "\n",
        "Here we provide a simple implementation of an adapter layer in pax that could be\n",
        "used to generate the virtual tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkrGolzR1Vde"
      },
      "source": [
        "## Install relevant packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SAYzENwyQ5F"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install praxis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZGC059108L7"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax import numpy as jnp\n",
        "import numpy as np\n",
        "from praxis import base_layer\n",
        "from praxis import layers\n",
        "from praxis import pax_fiddle\n",
        "from praxis import py_utils\n",
        "from praxis.layers import linears\n",
        "\n",
        "JTensor = base_layer.JTensor\n",
        "NestedMap = py_utils.NestedMap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7iukggk3iRk"
      },
      "source": [
        "## Define Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vnMFT6E1AjD"
      },
      "outputs": [],
      "source": [
        "# Size of the token embeddings used in the LLM transformer.\n",
        "MODEL_DIM = 128\n",
        "\n",
        "# Dimension of the input data to the adapter.\n",
        "DATA_INPUT_DIM = 40\n",
        "\n",
        "# Number of virtual tokens to use in the adapter.\n",
        "NUM_VIRTUAL_TOKENS = 10\n",
        "\n",
        "# Number of MLP layers to use in the data adapter.\n",
        "NUM_MLP_LAYERS = 5\n",
        "\n",
        "# Batch size used in training.\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Random seed\n",
        "RANDOM_SEED = 123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k__ZJxUL1DJt"
      },
      "outputs": [],
      "source": [
        "class SimpleAdapter(base_layer.BaseLayer):\n",
        "  \"\"\"A simple adapter layer for vector input data.\"\"\"\n",
        "\n",
        "  # The dimension of input data.\n",
        "  input_dims: int = 1\n",
        "  # The dimension of the token embeddings.\n",
        "  token_embedding_dim: int = 128\n",
        "  # The number of virtual tokens to allocate to data encoding.\n",
        "  num_virtual_tokens: int = 10\n",
        "  # Number of layers for the MLP used in the adapter.\n",
        "  num_layers: int = 5\n",
        "\n",
        "  def setup(self) -\u003e None:\n",
        "    ffn_p = pax_fiddle.Config(\n",
        "        linears.FeedForward,\n",
        "        name='ffn',\n",
        "        input_dims=self.input_dims,\n",
        "        output_dims=self.token_embedding_dim * self.num_virtual_tokens,\n",
        "    )\n",
        "    mlp_p = pax_fiddle.Config(\n",
        "        linears.MLPBlock,\n",
        "        num_layers=self.num_layers,\n",
        "        name='mlp',\n",
        "        activate_final=False,\n",
        "        ff_tpl=ffn_p,\n",
        "    )\n",
        "    self.create_child('mlp_layer', mlp_p)\n",
        "\n",
        "  def __call__(self, input: JTensor):\n",
        "    \"\"\"Apply the layer.\"\"\"\n",
        "    batch_size, _ = input.shape\n",
        "    x = self.mlp_layer(input)\n",
        "    return jnp.reshape(\n",
        "        x, [batch_size, self.num_virtual_tokens, self.token_embedding_dim]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MMm_lfp34Fo"
      },
      "source": [
        "## Test the layer with synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS6GnRW11IxP"
      },
      "outputs": [],
      "source": [
        "_adapter_config = pax_fiddle.Config(\n",
        "    SimpleAdapter,\n",
        "    name='adapter_config',\n",
        "    input_dims=DATA_INPUT_DIM,\n",
        "    token_embedding_dim=MODEL_DIM,\n",
        "    num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
        "    num_layers=NUM_MLP_LAYERS,\n",
        ")\n",
        "g_adapter_layer = base_layer.instantiate(_adapter_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VIjUz0M1LCD",
        "outputId": "666fd96c-b4b4-47be-a1b2-f5a663a0997c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We expect 8 samples by 40\n",
            "_inputs: (8, 40)\n",
            "\n",
            "We expect 8 samples by 10 tokens by 128 dim.\n",
            "_outputs: (8, 10, 128)\n"
          ]
        }
      ],
      "source": [
        "# We create an input matrix of size BATCH_SIZE by DATA_INPUT_DIM.\n",
        "# For example, with the default constants this represents\n",
        "# a matrix of 8 samples where each has a data input vector of size 40.\n",
        "_inputs_np = np.random.normal(\n",
        "    size=(\n",
        "        BATCH_SIZE,\n",
        "        DATA_INPUT_DIM,\n",
        "    )\n",
        ")\n",
        "_inputs = jnp.asarray(_inputs_np)\n",
        "\n",
        "print(f'We expect {BATCH_SIZE} samples by {DATA_INPUT_DIM}')\n",
        "print(f'_inputs: {_inputs.shape}')\n",
        "print('')\n",
        "\n",
        "\n",
        "with base_layer.JaxContext.new_context():\n",
        "  _prng_key = jax.random.PRNGKey(seed=RANDOM_SEED)\n",
        "  _prng_key, _subkey = jax.random.split(_prng_key)\n",
        "  _pax_initial_vars = g_adapter_layer.init(\n",
        "      {base_layer.PARAMS: _prng_key, base_layer.RANDOM: _subkey}, _inputs\n",
        "  )\n",
        "  _pax_initial_vars = NestedMap(_pax_initial_vars)\n",
        "  _outputs = g_adapter_layer.apply(\n",
        "      _pax_initial_vars, _inputs, rngs={base_layer.RANDOM: _subkey}\n",
        "  )\n",
        "\n",
        "# After pushing the input matrix through the adapter we expect that\n",
        "# the output of the adapter should be BATCH_SIZE by NUM_VIRTUAL_TOKENS\n",
        "# by MODEL_DIM or the token embedding size.\n",
        "print(\n",
        "    f'We expect {BATCH_SIZE} samples by {NUM_VIRTUAL_TOKENS} tokens by'\n",
        "    f' {MODEL_DIM} dim.'\n",
        ")\n",
        "print(f'_outputs: {_outputs.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jledM68D4f_Z"
      },
      "source": [
        "## Where to go next\n",
        "\n",
        "The output of the adapter layer can be concatenated with the input to the LLMs.\n",
        "The details of how this is done will depend on the specific implementation, but\n",
        "in general it should be quite straightforward."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
